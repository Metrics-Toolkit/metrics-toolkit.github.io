<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="utf-8">
  <title>Metrics Toolkit</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="generator" content="Hugo 0.65.2" />
  
  <!-- ** CSS Plugins Needed for the Project ** -->
  
  <!-- Bootstrap -->
  <link rel="stylesheet" href="https://metrics-toolkit.github.io/plugins/bootstrap/bootstrap.min.css">

  <!-- themefy-icon -->
  <link rel="stylesheet" href="https://metrics-toolkit.github.io/plugins/themify-icons/themify-icons.css">

  <!--Favicon-->
  <link rel="icon" href="https://metrics-toolkit.github.io/images/favicon.png" type="image/x-icon">

  <!-- fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open%20Sans:300,400,700&display=swap" rel="stylesheet">
  
  <style>
  :root{
    --primary-color:#005166;
    --body-color:#f9f9f9;
    --text-color:#636363;
    --text-color-dark:#242738;
    --white-color:#ffffff;
    --light-color:#f8f9fa;
    --font-family:Open Sans;
  }
  </style>

<!-- Main Stylesheet -->

<link href="https://metrics-toolkit.github.io/css/style.min.css" rel="stylesheet" media="screen"/>
  
<!-- ** JS Plugins Needed for the Project ** -->

<!-- jquiry -->
<script src="https://metrics-toolkit.github.io/plugins/jquery/jquery-1.12.4.js"></script>

<!-- jquary ui -->
<script src="https://metrics-toolkit.github.io/plugins/jquery/jquery-ui.js"></script>

<!-- Bootstrap JS -->
<script src="https://metrics-toolkit.github.io/plugins/bootstrap/bootstrap.min.js"></script>

<!-- match-height JS -->
<script src="https://metrics-toolkit.github.io/plugins/match-height/jquery.matchHeight-min.js"></script>



</head>
  
  <body>
    
    <!-- header -->
    <header class="banner overlay bg-cover" data-background="https://metrics-toolkit.github.io/images/metrics-banner.png">
      <nav class="navbar navbar-expand-md navbar-dark">
  <div class="container">
    <a class="navbar-brand px-2" href="/">
      
      
      
      Metrics Toolkit
      
    </a>
    <button class="navbar-toggler border-0" type="button" data-toggle="collapse" data-target="#navigation"
      aria-controls="navigation" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse text-center" id="navigation">
      <ul class="navbar-nav ml-auto">
        <li class="nav-item">
          <a class="nav-link text-dark" href="/">Home</a>
        </li>
        
        
        <li class="nav-item">
          <a class="nav-link text-dark" href="/metrics">Browse Metrics</a>
        </li>
        
        
        
        <li class="nav-item">
          <a class="nav-link text-dark" href="/resources">Resources</a>
        </li>
        
        
        
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle text-dark" href="#" role="button" data-toggle="dropdown"
            aria-haspopup="true" aria-expanded="false">
            about
          </a>
          <div class="dropdown-menu">
            
            <a class="dropdown-item" href="/about">About</a>
            
            <a class="dropdown-item" href="/about/editorial-board">Editorial Board</a>
            
            <a class="dropdown-item" href="/about/schema">Schema</a>
            
          </div>
        </li>
        
        
      </ul> 
      
    </div>
  </div>
</nav> 
      <!-- banner -->
<div class="container section">
	<div class="row">
		<div class="col-lg-8 text-center mx-auto">
			<h1 class="text-white mb-3">Metrics Toolkit</h1>
			<p class="text-white mb-4">Helping you navigate the research metrics landscape</p>
			<div class="position-relative">
				<input id="search" class="form-control" placeholder="Search">
				<i class="ti-search search-icon"></i>
				
				
				
				
				<script>
					$(function() {
					var projects = [
						
						{
							value: "About the Metrics Toolkit",
							label: "<p>We developed the Metrics Toolkit to help scholars and evaluators understand and use citations, web metrics, and altmetrics responsibly in the evaluation of research.\nThe Metrics Toolkit provides evidence-based information about research metrics across disciplines, including how each metric is calculated, where you can find it, and how each should (and should not) be applied. You’ll also find examples of how to use metrics in grant applications, CV, and promotion packages.\nRoadmap for 2020 – 2021 2020  Develop a print version of the Toolkit Revise our metrics schema Develop and implement an outreach plan Develop sample use cases to facilitate responsible use of metrics in promotion \x26amp; tenure packages, funding proposals, etc.  2021  Change our web stack to streamline publishing workflows and make it easier for others to reuse the content Seek out funding for continued expansion of and improvements to the site  Connect  Email Github Twitter  </p>",
							url:"https:\/\/metrics-toolkit.github.io\/about\/"
						},
						
						{
							value: "Metric Description Schema",
							label: "<p>    Field Field Description   Name The name of the metric (e.g., h-index)   Can apply to Indication of to what the metric is applied (e.g. an individual researcher, a paper, journal, etc).   Metric definition Narrative definition of what the metric calculates   Metric calculation Mathematical\/quantitative definition of the metric, when available   Data sources Data sources utilized to calculate metric   Appropriate use cases Lists the attention\/impact related questions the metric can help address   Limitations Describes the limitations of the metric, as identified in the literature   Inappropriate use cases Describes the attention\/impact related questions the metric should not be used to address   Available sources   The individuals or organizations that maintain and publish the metric, with links to those sources.     Transparency   The ability to verify source data, and the availability of documentation about how a metric is derived     Website Link to the metric\x27s official website, when appropriate   Timeframe Time scope or coverage used in calculating the metric   Last updated The date the metric description was last reviewed or revised    </p>",
							url:"https:\/\/metrics-toolkit.github.io\/about\/schema\/"
						},
						
						{
							value: "Metrics Toolkit",
							label: "<p></p>",
							url:"https:\/\/metrics-toolkit.github.io\/"
						},
						
						{
							value: "Metrics Toolkit Editorial Board",
							label: "<p>The Metrics Toolkit Editorial Board is comprised of scientometrics experts well versed in research impact metrics and their real-world applications. Editors make a one-year commitment to maintain and develop new Toolkit content by staying abreast of the relevant literature and joining monthly Editorial Board calls. In collaboration with the Toolkit’s co-founders, Editorial Board members work to ensure the Metrics Toolkit provides current and evidence driven information about research metrics.\n          Manolis Antonoyiannakis is an Associate Editor and Bibliostatistics Analyst at the American Physical Society, and an Adjunct Associate Research Scientist at the Department of Applied Physics \x26amp; Applied Mathematics at Columbia University. He received his PhD in physics from Imperial College London. Manolis has extensive experience on scholarly publishing and research assessment, having handled the peer review of more than 10,000 manuscripts in the Physical Review journals. He is interested in the science of science, information science, and scientometrics. More specifically, he is interested in how data science can be used to study scientific productivity, creativity, behavior, and impact, and he is keen to contribute toward a more sensible use of scholarly metrics that is anchored on solid statistical grounds.    Robin Champieux is the Director of Education, Research and Clinical Outreach at Oregon Health Sciences University, where she leads the Library’s scholarly communication and research data services. Her work and research is focused on enabling the creation, reproducibility, accessibility, and impact of digital scientific materials. She is a co-founder of the Metrics Toolkit and Awesome Libraries. Robin was a 2018 NLM\/AAHSL Leadership Fellow, and is a graduate of the Harvard Leadership Institute for Academic Librarians. You can find Robin on Twitter or Github.    Robin Chin Roemer is the Head of Instructional Design \x26amp; Outreach Services at the University of Washington Libraries in Seattle. She is the author of numerous works related to research impact, including the 2015 handbook Meaningful Metrics: A 21st Century Librarian’s Guide to Research Impact, Bibliometrics, and Altmetrics (ACRL Press).    Heather Coates is the Digital Scholarship \x26amp; Data Management Librarian in the IUPUI University Library, Interim Co-Director of the Center for Digital Scholarship, and a co-founder of the Metrics Toolkit. In her work, Heather supports open dissemination of scholarship in many forms and the responsible use of research metrics as evidence for career advancement. Heather’s research interests include open science, open data, and open access as a means to improve the integrity of the scholarly record and advance our understanding of the world. You can find Heather on Twitter, Github, ORCID, or coateshl.com.    Rodrigo Costas is a senior researcher at the Centre for Science and Technology Studies (CWTS) at Leiden University, and an Extraordinary Associate Professor at the Centre for Research on Evaluation, Science and Technology (CREST) of Stellenbosch University. His areas of expertise include the fields of information science, scientometrics, and social media metrics. At CWTS he leads research in ‘altmetrics’, focused on developing new approaches to studying the interactions between social media and science. Rodrigo’s interests also include the development of advanced scientometric studies of individual scholars and the study of funding acknowledgments as a new source of scientometric data. He holds a PhD in Library and Information Science.    Karen Gutzman is the Digital Innovations Specialist at Galter Health Sciences Library \x26amp; Learning Center at Northwestern University, where she supports and implements programs that increase awareness about digital scholarship and issues in the digital environment among faculty, researchers, and students at Feinberg School of Medicine. Previously, Karen was the Impact and Evaluation Librarian where she supported the understanding, assessment, visualization, and reporting of impactful outcomes of research and clinical care efforts. Karen is particularly interested in using information visualization to facilitate stronger comprehension of data in the assessment process. She keeps an updated collection of her works in DigitalHub , the institutional repository for Northwestern Medicine.    Stacy Konkiel is the Director of Research Relations at Altmetric and Dimensions, and a co-founder of the Metrics Toolkit. Stacy’s research interests include incentives systems in academia and informetrics, and she has written and presented widely about altmetrics, Open Science, and library services. Stacy was a co-founder of the HuMetricsHSS initiative. Previously, Stacy worked with teams at Impactstory, Indiana University \x26amp; PLOS. You can learn more about Stacy at stacykonkiel.org or on Github.    Barbara S. Lancho Barrantes is a Bibliometrician leading the Bibliometrics service at the University of Leeds. She has a PhD in Bibliometrics developed within the SCImago research group. In her previous role she has been a Postdoctoral researcher in Scientometrics at Tecnologico de Monterrey (Mexico). Her scientific production mainly focuses on citation flows among countries and disciplines. She has published in journals and participated in conferences in the bibliometrics area. Recently she has joined the LIS-Bibliometrics committee as a Competencies Development Officer and to the Innovative Metrics Working Group – LIBER.    Maria Manuel Borges is an Associate Professor in Information Science at the University of Coimbra and co-coordinator of the Digital Humanities Group at the Centre for 20th Century Interdisciplinary Studies – CEIS20 of the University of Coimbra. She is also a member of the editorial board of national and international journals and Associate Editor of the Directory of Open Access Journals (DOAJ). She is a member of the Interministerial Working Group, sub-group Research Assessment, whose mission is, among others, to advise the Portuguese Ministry of Science, Technology and Higher Education about the strategic orientation for the National Open Science initiative. She is also a member of the Research Data Alliance and a member of the board supported by the University of Coimbra of the node RDA.pt.    Alisa Surkis is the Assistant Director for Research Data and Metrics and Vice Chair for Research at the NYU Health Sciences Library. She leads efforts at NYU Langone Health to track and assess the impact of publications, including supporting a database of faculty publications and leading a redesign for institutional bibliometrics dashboards. Her team provides custom reports and has developed a tool for self-service reports on the impact, topic, and collaborations of publications for individuals, research teams, departments, and the institution. Her team also maintains an institutional data catalog, and serves as a locus for education on collecting, managing, analyzing, visualizing, and sharing data. Alisa’s interests include assessing the research impact of data sharing and assessing the translation of research through publications.    Rebecca Welzenbach is the Research Impact Librarian at the University of Michigan Library, where she helps scholars communicate the importance of their work and coordinates the library’s role in research impact activities on campus. Prior to that, she contributed to developing and sustaining U-M Library’s open access publishing and digital scholarship initiatives in a variety of roles. She earned her MSI from the University of Michigan School of Information in 2009.    Zohreh Zahedi (ORCID ID: https:\/\/orcid.org\/0000-0001-5801-1886 ) is a visiting researcher at the Centre for Science and Technology Studies (CWTS) of Leiden University in the Netherlands. She obtained her PhD degree from CWTS, Leiden University. Her PhD thesis centers around the value of using social media metrics (altmetrics) for research evaluation. Zohreh’s current research line focuses on studying the interactions and communications between social media users and contents shared on social media platforms. Beside doing research, she is interested in understanding how research metrics could inform policy making and what are the best practices that could inform responsible use of research metrics in research assessments. Zohreh has contributed to the development of the US National Information Standard Organization code of conduct for altmetrics data. She is part of the editorial board of Journal of Altmetrics.     Past Metrics Toolkit Board Members Isidro F. Aguillo is the head of the Cybermetrics Lab, at the Institute de Public Goods and Policies (IPP) of the Spanish National Research Council (CSIC). He is the editor of the Rankings Web (Webometrics) and is the founder and editor of the journal “Cybermetrics”. Isidro served on the Metrics Toolkit’s inaugural board, 2018-2019.\nAndréa Gonçalves do Nascimento is a librarian and independent consultant in scholarly publishing and academic impact. She is the author of the book “Altmetrics for librarians: practical guide to alternative metrics for the assessment of scientific output”. Andréa served on the Metrics Toolkit’s inaugural board, 2018-2019.\n</p>",
							url:"https:\/\/metrics-toolkit.github.io\/about\/editorial-board\/"
						},
						
						{
							value: "Resources",
							label: "<p>There are so many great resources out there! We’ve selected a brief list to get you started without overwhelming you with too many options.\nCheck out the succinct handouts and guides at WhatAreAltmetrics?\nWatch this video guide to altmetrics for beginners.\nStill want to know more? This reading list from ResponsibleMetrics.org is really good.\nBooks \x26amp; Book Chapters  Beyond bibliometrics: harnessing multidimensional indicators of scholarly impact by Blaise Cronin \x26amp; Cassidy Sugimoto (eds.) Priem, J. (2014). ‘Altmetrics’ (Open access chapter from Beyond Bibliometrics: Harnessing Multidimensional Indicators of Scholarly Impact) The evaluation of research by scientometric indicators by Vinkler, P. Measuring Research: What Everyone Needs to Know by Cassidy Sugimoto and Vincent Larivière (2018)  Articles  Hicks, D., Wouters, P., Waltman, L., de Rijcke, S., and Rafols, I. (2015) ‘Bibliometrics: The Leiden Manifesto for research metrics.’ Nature. 520, 429-431. doi:10.1038\/520429a Priem, J., Taraborelli,, D., Groth, P. and Neylon, C. (2010) Altmetrics: a manifesto.  Costas, R., Zahedi, Z. and Wouters, P. (2014) ‘Do altmetrics correlate with citations? Extensive comparison of altmetric indicators with citations from a multidisciplinary perspective.’ JASIST 66(10). (preprint) Thelwall, M. (2016). Interpreting correlations between citation counts and other indicators. Scientometrics, 108(1), 337-347. (preprint)  Websites  Responsible Metrics NISO Alternative Assessment Metrics Initiative Snowball Metrics  This page has been adapted from WhatAreAltmetrics.com under a CC-BY license.\n</p>",
							url:"https:\/\/metrics-toolkit.github.io\/resources\/"
						},
						
						{
							value: "Altmetric Attention Score",
							label: "<p>   Field Field Value     Name Altmetric Attention Score   Can Apply To Journal articles, books, and any research output deposited to a repository that the company tracks   Metric Definition \x26ldquo;The Altmetric Attention Score is an automatically calculated, weighted count of all of the attention a research output has received [online, in sources tracked by Altmetric].\x26rdquo;   Metric Calculation The AAS takes into account the volume of attention received by a research output across a number of online attention sources (e.g. Twitter, Pubpeer, etc). Each source is weighted by the company. The AAS weighting also takes into account whether the author of a mention of a research output regularly posts about scholarly articles.   Data Sources News articles, Blogs, Twitter, Facebook, Sina Weibo, Wikipedia, Policy Documents (per source), Q\x26amp;A, F1000, Publons, Pubpeer, YouTube, Reddit, Pinterest, LinkedIn, Open Syllabus, Google\x2b. Mendeley readers, citation counts and CiteULike bookmarks do not count towards the score.   Appropriate Use Cases The AAS is best used by individual researchers to understand the overall volume of attention that research has received online. Individuals may also use the “Score in Context” (found on Altmetric details pages) to understand how a research output’s score compares to other scores. The AAS may also be used by publishers and institutions to group the attention received by its published and\/or produced research in order to monitor and benchmark its reach.   Limitations The AAS does not take into account the sentiments of mentions made about research objects, and thus does not help one understand the positive nor negative attention that a piece of research has received. According to Lockwood (2016), “Article titles with result-oriented positive framing and more interesting phrasing receive higher Altmetric [attention] scores”, and the same goes with articles with catchy titles (Poplasen \x26amp; Grgic, 2016). Conflicting research exists as to whether the number of collaborators on a paper may increase or decrease an AAS (Didegah, 2016; Haustein, Costas \x26amp; Larivière, 2015). International collaborations may increase an AAS (Didegah, 2016). Institutional prestige of authors reportedly does not affect an AAS (Didegah, 2016). Legitimate self-promotion by authors may artificially increase an AAS (Adie, 2013). Journal impact factor and article accessibility may positively influence an article’s AAS; “publications from Social Sciences \x26amp; Humanities have more mentions on Twitter and Facebook…[thus impacting an article’s AAS] than publications from both Engineering \x26amp; Technology and Medical \x26amp; Natural sciences” (Didegah, 2016). Differences in coverage and frequency of updates influence differences in counts of altmetric indicators (Bar-Ilan \x26amp; Halevi, 2017). Studies show that there is very little overlap between very highly cited papers and those that receive high altmetric scores (Banshal et al, 2018; Poplasen \x26amp; Grgic, 2016).   Inappropriate Use Cases The AAS should not be used as a direct measure of research impact of any kind, or quality.   Available Metric Sources The AAS can be found in all products offered by Altmetric, including the free researcher bookmarklet and on many journal publisher websites. The Dimensions database also includes the Altmetric Attention Score for the articles it indexes.   Transparency It is not possible to fully audit the AAS, as the weighting of the score depends upon non-public, company-assigned \x26ldquo;tiers\x26rdquo; for news sources, Twitter users, and some other sources that mention a research output.   Website The donut and Altmetric Attention Score, How is the Altmetric Attention Score calculated, Sources of attention   Timeframe Mostly post-2011. For more information on the coverage dates for sources contributing to the AAS, visit the Altmetric website.    </p>",
							url:"https:\/\/metrics-toolkit.github.io\/metrics\/almetric_attention_score\/"
						},
						
						{
							value: "Blog mentions",
							label: "<p>   Field Field Value     Name Blog Mentions   Can Apply To Primarily articles, books, and other scholarly items with persistent identifiers, such as a DOI, url, or handle   Metric Definition The number of times a scholarly output has been linked to from a blog.   Metric Calculation Blog mentions are comprised of raw counts of links to outputs, from blogs. Some services track links only for items with a persistent identifier.   Data Sources Links from both scholarly and general interest blogs are tracked, though coverage varies between altmetrics services.   Appropriate Use Cases Discussions on blogs have been found to have a slight correlation to later citations. They are not a sure-fire indicator for later citations. Use blog mentions only to learn what other researchers or members of the public are saying about a piece of research. The number of blog mentions is less important than what is being said.   Limitations Given their unvetted nature, it is risky to assume that a link to a piece of research from any blog–even a scholarly one–constitutes quality criticism.   Inappropriate Use Cases Blog mentions should not be interpreted as a direct measure of quality or impact, even amongst researcher blog networks.   Available Metric Sources Altmetric, PlumX, Impactstory Profiles, CrossRef Event Data (for items with DOIs, on Wordpress.com-hosted blogs)   Transparency In all altmetrics services in which blog mentions are available, one can access the full-text of the blog mentions, making this a relatively transparent metric. However, no altmetrics service makes their full list of blogs tracked available.   Website n\/a   Timeframe Altmetric began tracking blog mentions in October 2011 (meaning Impactstory Profiles coverage spans this time frame, as well). PlumX does not share information on its coverage dates for blog mentions.    </p>",
							url:"https:\/\/metrics-toolkit.github.io\/metrics\/blog_mentions\/"
						},
						
						{
							value: "Book chapters",
							label: "<p></p>",
							url:"https:\/\/metrics-toolkit.github.io\/categories\/book-chapters\/"
						},
						
						{
							value: "Books",
							label: "<p></p>",
							url:"https:\/\/metrics-toolkit.github.io\/categories\/books\/"
						},
						
						{
							value: "Categories",
							label: "<p></p>",
							url:"https:\/\/metrics-toolkit.github.io\/categories\/"
						},
						
						{
							value: "Citation percentiles and \x27Highly Cited\x27 labels",
							label: "<p>   Field Field Value     Name Citation percentiles and “Highly Cited” labels   Can Apply To Journal articles   Metric Definition \x26ldquo;The position of a paper or group of papers with respect to other papers in a given discipline, country, and\/or time period, based on the number of citations they have received. Expressed as a percentile or awarded a “Highly Cited” honorific based upon percentile rankings.   Metric Calculation Citation percentiles are defined as “[a] percentile-based bibliometric indicator is an indicator that values publications based on their position within the citation distribution of their field. The most straightforward percentile-based indicator is the proportion of frequently cited publications, for instance the proportion of publications that belong to the top 10% most frequently cited of their field.”    Note that depending upon the data source, percentiles can be expressed differently, in that an “inverted percentile” (e.g. 1%) and regular percentile (e.g. 99th percentile) are interpreted to mean the same thing.    Clarivate Analytics’s InCites platform has been credited for classifying percentiles in a particularly clear and specific manner (i.e. by showing not only the top 1% of papers, but also those in the top 0.1% and 0.01%), giving “a better indication of skew and ranking among the top-cited publications”.    SciVal provides a slightly different reference set than other providers: percentiles for each year are calculated against the entire Scopus database by default, and different comparison axes (called “data universes”), such as publication type, can be used for further comparison. In SciVal, “Outputs in Top Percentiles can only be calculated for the current year from the first data snapshot on or after 1 July. It will be displayed as a null value until this date is reached”.   Data Sources Counts and percentiles are calculated using citation data from Web of Science or Scopus.   Appropriate Use Cases Percentiles based jointly upon subject area, document type (i.e. research or review articles), and year of publication are described as being the most appropriate means of comparison between journal articles or groups of journal articles. Essentially they follow the same logic as any other field-normalized citation indicator, with the advantage that percentile-based citation indicators are less prone to be influenced by outliers.   Limitations Percentile-based indicators are based on citations, so they inherit all of citations’ limitations, including that a number of factors that can influence the citation rates for publications within subject areas and year of publication, including gender, the number of co-authors, the nationalities of authors, and outliers (very high or low cited publications). As such, these percentiles should be interpreted with care.    Though in theory simple to calculate, there are a number of possible variations in formulae used to calculate percentiles; as such, percentiles are only directly comparable when the same calculation is used for each.    Field-normalization has its own inherent limitations that impact the use of percentiles. The accuracy of field classification will affect the usefulness of a field’s percentiles, as will cases where a single paper may be assigned to several different fields (each of which can have highly variable median citation counts).   Inappropriate Use Cases In general, percentiles based upon very small comparison sets are ill-advised, as just one or two additional publication may substantially increase or decrease the share of highly cited publications.    Percentiles should not be used to make judgements about papers using reference sets that compare papers with other papers published in the same journal. Comparisons of groups of papers (e.g. for an individual’s body of work) should only be made given a sufficiently large corpus, as in the case of senior researchers. When evaluating heterogenous or interdisciplinary groups of papers, it’s possible that important subtopics with low citation densities may be left out.   Available Metric Sources Essential Science Indicators, InCites, SciVal   Transparency The specific formulae and source citation data used to calculate percentiles can vary between providers and are not always publicly documented.   Website n\/a   Timeframe Differs between providers   Further reading Bornmann L., Leydesdorff L., and Mutz R. (2013). The Use of Percentiles and Percentile Rank Classes in the Analysis of Bibliometric Data: Opportunities and Limits. Journal of Informetrics, 7(1),158–65. https:\/\/doi.org\/10.1016\/j.joi.2012.10.001    Bornmann, L., \x26amp; Marx, W. (2013). How good is research really? EMBO Reports, 14(3), 226–230. https:\/\/doi.org\/10.1038\/embor.2013.9    </p>",
							url:"https:\/\/metrics-toolkit.github.io\/metrics\/highlycited_awards\/"
						},
						
						{
							value: "Citations, Articles",
							label: "<p>   Field Field Value     Name Citations, Articles   Can Apply To Journal articles and preprints   Metric Definition The number of times that a journal article or preprint has appeared in the reference list of other articles and books.   Metric Calculation Many citation databases use a combination of text-mining and manual classification to build their lists of citations, based upon the reference lists of articles and books that they index. However, the scope of these databases varies, with Web of Science being the most selective (in terms of the quantity of journals and disciplines covered) and Google Scholar being the least selective (indexing a great deal of non-peer-reviewed content and various research output types). Outside of Google Scholar and Microsoft Academic, it is difficult to track citations to preprints.   Data Sources Citations are mined from the references sections of articles published in a manually curated list of journals, or in the case of Google Scholar, from any scholarly web domain.   Appropriate Use Cases There are many diverse reasons why scholars cite each others’ work, so it’s impossible to say that there is one way that citations should be interpreted. The closest one can get is to say that citations are a measure of influence amongst other scholars, and that influence can sometimes be negative (especially in the humanities). Citations to journal articles are generally better applied to the evaluation of STEM research, given the dearth of coverage of humanities, arts, and social sciences research in most citation databases.   Limitations One needs to read the context of a citation to understand its true meaning. Many factors can impact citation counts including database coverage, differences in publishing patterns across disciplines, citation accrual times, self-citation rates, the age of the publication, observation period, or journal status.    Database coverage. Citation databases like Web of Science and Scopus have been recognized to have limited coverage of humanities, arts, and social sciences research as compared to the sciences, as well as limited coverage of local and specialized journals, especially those written in languages other than English.    Discipline-specific publishing norms. Moreover, differences in authorship norms between disciplines–some fields regularly have dozens of authors for a paper, where others tend to have single-author papers–meaning that citations cannot always measure the full extent of an author’s contributions towards a work. Citations accrue at different rates across disciplines, depending on the publishing volume and other norms. For example, a paper in oncology may accrue 10 citations in the first year after publication, while a paper in philosophy may take several years to accrue as many citations.    Self-citation rates. Author self-citation is an essential part of scholarly communication and can impact citation counts. Identifying the number of self-citations provides supplementary information about the citations themselves.    Age of publication. Citations are impacted by the age of the paper. More recently published papers have had less time to accrue citations. Most papers \x26ldquo;receive a growing number of citations to arrive at a peak somewhere between two and six years after publication before the citation count decreases, while some receive most of the citations within a year or two, others are cited constantly for a long period, and still others remain unmarked before a sudden wave of citations arrives seven or ten years afterwards.\x26quot;    Observation Period. If limiting the number of years from which a citation is counted, the overall citation count may decrease for a publication.   Inappropriate Use Cases Citation counts should never be interpreted as a direct measure of research quality and should not be used as a measure of positive reputation for individual researchers. Citation counts should not be used to compare papers of different age (i.e. publication year), type (i.e. articles, reviews, etc.) or subject areas. A metric more suited for this type of comparison is the Field Normalized Citation Impact.   Available Metric Sources Popular sources of citations to journal articles include Dimensions, Google Scholar, Scopus, Web of Science, and Microsoft Academic.   Transparency Citations are only as transparent as the availability of the citing article or book allows them to be. One may not always be able to read citations in context, given the prevalence of subscription journals to which reviewers are not guaranteed access. Most databases that report citations report the full list of citing articles, at the very least, linking through to full-text articles where possible (even if only for subscribing institutions).   Website n\/a   Timeframe In theory, it is possible to track citations to journal articles as far back as the advent of the scientific journal. While some coverage exists prior to 1900, coverage for Scopus and Web of Science is strongest for 1900 – present.    </p>",
							url:"https:\/\/metrics-toolkit.github.io\/metrics\/citations_articles\/"
						},
						
						{
							value: "Citations, Books and book chapters",
							label: "<p>   Field Field Value     Name Citations, Books and Book Chapters   Can Apply To Scholarly monographs and trade books, and chapters of scholarly monographs   Metric Definition The number of times that a book or chapter has appeared in the reference list of other articles and books.   Metric Calculation Many citation databases use a combination of text-mining and manual classification to build their lists of citations. However, the scope of these databases varies, with some databases indexing citations to far fewer books than others.   Data Sources Book Citation Index (available through Web of Science), Google Books, Google Scholar, Scopus   Appropriate Use Cases As with journal articles, there are many, many reasons why scholars cite each others’ work, so it’s impossible to say that there is one way that citations should be interpreted. The closest one can get is to say that citations are a measure of influence amongst other scholars, and that influence can sometimes be negative (especially in the humanities). Citations to journal articles are generally better applied to the evaluation of STEM research, given the dearth of coverage of humanities, arts, and social sciences research in most citation databases.   Limitations One needs to read the context of a citation to understand its true meaning. In general, it is more difficult to find comprehensive citations to a monograph or to its chapters than for a journal article, due to the limited scope of major book citation databases. The current means of calculating citations (tracking a monograph’s appearance in a reference list) do not account for how often a monograph is cited in another text. For example, some works are cited many times throughout a text and are thus central to another scholars’ research, while other works are cited only once. Most citation databases that include books share the limitations of other citation databases: they favor English-language research and newer monographs, missing local and regional research published in other languages, as well as older monographs.   Inappropriate Use Cases Citations should never be interpreted as a direct measure of quality. Raw citation counts should not be used as a measure of positive reputation for individual researchers.   Available Metric Sources Book Citation Index (available through Web of Science), Dimensions, Google Books, Google Scholar, Scopus   Transparency Citations are only as transparent as the availability of the citing article or book allows them to be. One may not always be able to read citations in context, given the prevalence of subscription journals to which reviewers are not guaranteed access. Most databases that report citations report the full list of citing articles and books, at the very least, linking through to full-text articles and books where possible (even if only for subscribing institutions).   Website n\/a   Timeframe In theory, it is possible to track citations to books published hundreds of years ago. But in practice, most databases only track citations for books published in the 20th century and beyond.    </p>",
							url:"https:\/\/metrics-toolkit.github.io\/metrics\/citations_books_bookchapters\/"
						},
						
						{
							value: "Citations, Data",
							label: "<p>   Field Field Value     Name Citations, Data   Can Apply To Research data sets and journal articles that describe them.   Metric Definition The number of times a journal article or book has referenced a data set.   Metric Calculation Data citations are sometimes collected only in the formal sense (i.e., with the data set being listed in the References section of a paper, alongside journal articles). They can also be calculated in the informal sense (i.e., linked to from within the Methods section of a paper). It varies from tool to tool.   Data Sources Web of Science via Data Citation Index, Google Scholar (rare)   Appropriate Use Cases Data citations should be used to understand how often research data has been reused in others’ studies, thereby indicating advancement of the field. Some fields (e.g.,crystallography and genomics) practice data citation at higher rates than others, and therefore evaluation of research from those fields may be more suitable scenarios for using data citations.   Limitations Data citation is still relatively rarely practiced, with only half of journals providing instruction for how to cite data and more than 88% of all Data Citation Index records going uncited. Lack for formal referencing poses a challenge for using data citations from tools that only count such formal references in their data citation metrics. Critics of data citation claim that data citations merely mimic existing metrics that do not “recognize all players involved in the life cycle of those data from collection to publication”. Disciplinary coverage in the Data Citation Index (as of 2017) is skewed, favoring the life sciences (48% of records) over the social sciences (20%), physical sciences (23%), arts \x26amp; humanities (7%), and multidisciplinary research (2%). Note that the Data Citation Index tracks citations for datasets and also related data studies (defined as “a description of studies or experiments held in repositories with the associated data which have been used in the data study”) as they are cited in articles indexed by the Web of Science databases. The availability of data should be taken into account when attempting to make comparisons for data citation rates against other data sets, as in some disciplines, open access data is cited at higher rates (up to 69% higher for cancer research).   Inappropriate Use Cases Citation counts should never be interpreted as a direct measure of quality. Raw citation counts should not be used as a measure of positive reputation for individual researchers.   Available Metric Sources Data Citation Index, Google Scholar (rare)   Transparency Varies by provider. The Data Citation Index is fully transparent regarding the data repositories it indexes. The Data Citation Index white paper, “Recommended practices to promote scholarly data citation and tracking” (n.d.), describes how the Web of Science can find properly formed citations to datasets in order to calculate citations for the DCI. Google Scholar can index any content that conforms to their formatting guidelines, but is designed to primarily index journal articles, monographs, and other “print” outputs.   Website n\/a   Timeframe In theory, data sets from any year can be referenced in scholarly literature. Google Scholar’s temporal scope is unknown. Data Citation Index includes citations to data from 1900 onwards.    </p>",
							url:"https:\/\/metrics-toolkit.github.io\/metrics\/citations_data\/"
						},
						
						{
							value: "Citations, Software",
							label: "<p>   Field Field Value     Name Citations, Software   Can Apply To Software packages and papers describing software packages   Metric Definition The number of times a piece of software or code (or a paper that describes software or code) has been cited as a resource in a journal article or book.   Metric Calculation Like data, software can be cited formally (in the references section of a paper) or informally (linked to from the methods section of a paper). Google Scholar searches the scholarly web for all mentions of a software package by name; Depsy searches PubMed Central Europe and ADS for mentions of a software package by name.   Data Sources Google Scholar   Appropriate Use Cases Citations to software can be interpreted to understand the influence of a software package, and in many cases the reuse of that software package in other researchers’ analyses.   Limitations Software packages are much less likely to be cited directly than articles about software packages are. Only around a third of citations to software are formal, so attempts to count software citations using existing tools may miss important mentions of software in research articles.   Inappropriate Use Cases Citations to software should not be interpreted to measure quality.   Available Metric Sources Google Scholar   Transparency While all sources link to the papers that cite software, the availability of these papers to the end user varies due to journal article paywalls. Though Google Scholar might link to a paper that cites a piece of software, the end user may not be able to read that paper to see the context of that citation, if they do not have a subscription to the journal in which the citing paper was published.   Website n\/a   Timeframe In theory, software of any age can be cited in the research literature.    </p>",
							url:"https:\/\/metrics-toolkit.github.io\/metrics\/citations_software\/"
						},
						
						{
							value: "Datasets",
							label: "<p></p>",
							url:"https:\/\/metrics-toolkit.github.io\/categories\/datasets\/"
						},
						
						{
							value: "Downloads, Articles",
							label: "<p>   Field Field Value     Name Downloads, Articles   Can Apply To Journal articles or other serial publications   Metric Definition A download is an event triggered by a user clicking on the download button, in contrast to simply viewing a web page.   Metric Calculation A count of downloads during a period of time.   Data Sources Publishers, subject repositories, institutional repositories, researcher profile systems, and altmetrics aggregators.   Appropriate Use Cases Article downloads can be used as a leading indicator or proxy for others intent to use, rather than actual usage. This may (or may not) be reflected in eventual citations.   Limitations Article downloads is not an accurate measure of consumption or how many people have read the item, despite the common tendency to equate downloads with usage. Downloaded files may languish unread in personal libraries (resulting in an inflated count of readership) or may be shared with a journal club or other individuals (resulting in an underestimate of readership). Tools that allow for automated crawling and downloading of content may also result in inaccurate counts. In order for web analytics tools to provide an accurate count, they need to be configured to monitor and count these events. The COUNTER Code of Practice provides a standard for processing such data, so it is more creditable and comparable, and includes a public registry of compliance. Standard analytics tools like Google Analytics may not see downloads when people connect directly to the file through Google or Google Scholar. Platforms commonly used for institutional repositories can count these from the server side, or use plug-ins to provide an accurate count. Finally, the correlation between citations and downloads may vary by discipline and institution.   Inappropriate Use Cases Article downloads should not be used as direct measures of usage, research quality, or impact.   Available Metric Sources Downloads from publisher sites, subject repositories, and institutional repositories are likely to be fairly accurate, while downloads from personal websites, blogs, and other platforms may be less reliable, depending on the configuration of the site and analytics tools.   Transparency Varies by source   Website n\/a   Timeframe Typically immediate, but there may be a reporting delay of a few hours up to 30 days, depending on the source.    </p>",
							url:"https:\/\/metrics-toolkit.github.io\/metrics\/downloads_articles\/"
						},
						
						{
							value: "Downloads, Books and book chapters",
							label: "<p>   Field Field Value     Name Downloads, Books and Book Chapters   Can Apply To Books and book chapters   Metric Definition A download is an event triggered by a user clicking on the download button, in contrast to simply viewing a web page.   Metric Calculation A count of downloads over a period of time.   Data Sources Publishers, subject repositories, and institutional repositories   Appropriate Use Cases Book and book chapter downloads can be used as a leading indicator or proxy for others intent to use, rather than actual usage. This may (or may not) be reflected in eventual citations.   Limitations The number of file downloads is not an accurate count of consumption or how many people have read the item, despite its common use in this way. Downloaded files may languish unread in personal libraries (resulting in an inflated count of readership) or may be shared with a journal club or other individuals (resulting in an underestimate of readership). Additionally, relationships between citations and downloads may vary by discipline and institution. This is compounded by tools that allow for automated crawling and downloading of content.    In order for web analytics tools to provide an accurate count, they need to be configured to monitor and count these events. he COUNTER Code of Practice provides a standard for processing such data, so it is more creditable and comparable, and includes a public registry of compliance. Standard analytics provided for commonly used platforms for professional portfolios (such as Wordpress.com) may not see downloads when people connect directly to the file through Google or Google Scholar. Platforms commonly used for institutional repositories can count these from the server side, or use plug-ins to provide an accurate count. | Inappropriate Use Cases | Book and book chapter downloads are not direct measures of usage, research quality, or impact. | Available Metric Sources | Varies by publisher and repository, if used | Transparency | Varies by source | Website | n\/a | Timeframe | Typically immediate, but there may be a reporting delay of a few hours up to a 30 days, depending on the source.\n</p>",
							url:"https:\/\/metrics-toolkit.github.io\/metrics\/downloads_books_bookchapters\/"
						},
						
						{
							value: "Downloads, Software",
							label: "<p>   Field Field Value     Name Downloads, Software   Can Apply To Research software, scripts, code snippets   Metric Definition File downloads over a period of time.   Metric Calculation Most downloads are calculated in a straightforward manner (where the number of downloads is simply reported as-is), but others account for–and remove–downloads initiated by bots.   Data Sources Though almost any web platform that can host files (including researcher websites), the most common and reliable sources of research software download statistics are those reported by software hosting sites (e.g. Bitbucket) or repositories (e.g. Figshare). Some altmetrics tools can report downloads for certain software hosting sites (e.g. PlumX).   Appropriate Use Cases Software downloads can be used as an indicator for the reuse of programming code. In some cases, it can be a proxy for the number of users.   Limitations Software is rarely formally cited, though it is often mentioned in publications. One study has found a weak, but statistically significant, correlation between Scopus citations and download counts for Google Code programs. SourceForge.net does not report downloads as of March 2017.   Inappropriate Use Cases Software downloads are not direct measures of usage, quality, or impact.   Available Metric Sources Google Code (defunct as of January 2016), Codeplex, Bitbucket, Launchpad, GitHub (API), Figshare, Zenodo   Transparency Varies by source   Website n\/a   Timeframe Typically immediate, but there may be a reporting delay of a few hours up to a 30 days, depending on the source.    </p>",
							url:"https:\/\/metrics-toolkit.github.io\/metrics\/downloads_software\/"
						},
						
						{
							value: "Field Weighted Citation Impact",
							label: "<p>   Field Field Value     Name Field Weighted Citation Impact   Can Apply To Primarily journal articles, but also other kinds of research outputs, such as book chapters and conference proceedings that are sufficiently covered by abstract and citation databases.   Metric Definition The Field Normalized Citation Impact (FNCI) is the ratio between the actual citations received by a publication and the average number of citations received by all other similar publications. The latter is referred to as the expected number of citations. Similar publications are ones in the same subject category, of the same type (i.e. article, review, book chapter, etc.), and of the same age (i.e. publication year).   Metric Calculation A FNCI is measured by dividing the number of citations a publication received by the average number of citations to publications in a database published in the same year, of the same type, and within the same subject category. When multiple publications are being considered, the ratio between the actual and average citations for each publication are calculated first. A typical indicator is the Mean Normalized Citation Score (MNCS), which is the mean result of all FNCI of all publications included in the analysis. Publications can also be assigned to more than one subject category. In these cases, usually the publication and its citation counts are proportionally distributed across the relevant subject categories.   Data Sources The FNCI is dependent on extensive citation and indexing information available in citation databases, such as Scopus and Web of Science. In addition to citation counts, these sources classify publications by year, type, and subject.   Appropriate Use Cases The FNCI was conceived to facilitate the benchmarking of citation performance across groups of different size, disciplinary scope, and age, such as research large groups, institutions, or geographic regions. It is meant to correct for the different disciplinary patterns of scholarly communication and publication age can have on non-normalized metrics, such as citation counts. The global mean of the FNCI is 1.0, so it is easy to compare a set of values to a benchmark. For example, an FNCI of 1.50 means 50% more cited than the world average; whereas, an FWCI of 0.75 means 25% less cited than the world average.   Limitations The FNCI is typically presented as a mean value (e.g. Mean Normalized Citation Score) for an aggregation of papers (e.g. individual scholars, a journal, a university, etc.), which can be strongly influenced by outliers. The distribution of citations across publications is often highly skewed. Most publication in a sample will receive relatively low citation attention, while a small set will accumulate high citation rates. Indicators based on highly cited publications are usually an alternative to mean-based indicators. Additionally, the FNCI may be sensitive to the field classification system chosen for the analysis, particularly when the classification is not at the publication-level but at the journal-level (e.g. Web of Science Subject Categories or the Scopus All Science Journal Classification (AJSC).   Inappropriate Use Cases Like for most citation analysis, citation-based metrics should not be interpreted as a direct measure of research quality.   Available Metric Sources Article level FNCI values are available in Scopus (Field Weighted Citation Index - FWCI), and other bibliometric sources (e.g. Web of Science, Dimensions, Google Scholar) provide possibilities of similarly calculated field-normalized citation based metrics.   Transparency The FNCI’s calculation is a well known methodology in bibliometric practice. The FNCI is dependent upon a publication’s classification by discipline, publication type, and year. While the year and type of publication are recorded and verifiable, discipline assignments are not always available.   Website N\/A   Timeframe The FNCI may be subject of different ‘citation windows.’ Typically, it uses citation data from the year of publication plus 3 years, although more extensive (i.e. larger than 3 years) or variable citation windows (i.e. considering all subsequent publication years available after the publication year) are possible.    </p>",
							url:"https:\/\/metrics-toolkit.github.io\/metrics\/field_weighted_citation_impact\/"
						},
						
						{
							value: "Github: Forks, collaborators, watchers",
							label: "<p>   Field Field Value     Name Github: Forks, Collaborators, Watchers   Can Apply To Any project content stored on Github.com, but primarily software and code.   Metric Definition Github “Forks” are created when a user makes a copy of a repository (i.e., a group of files). A “collaborator” is another Github user who is able to perform many actions on the files within the repository, including edits. “Watchers” are Github users who have asked to be notified of activity in a repository, but have not become collaborators. Watching a repository is similar to following an RSS feed to see changes.   Metric Calculation Forks, pulls, and commits are counted by the Git software running on the Github servers.   Data Sources Github.com   Appropriate Use Cases Forks, or copies, can indicate use or reuse of your repository, depending on the level of activity. For example, someone might fork your repository to suggest changes or as the basis for a new project. The identities of your Github collaborators may offer evidence of your engagement with particular communities, or possibly wide interest. Github watchers can indicate interest in your repository or project.   Limitations Simple counts of forks, collaborators, and watchers do not provide much information about the use of the repository files. However, exploring the specific activities and individuals can provide contextual information about who is engaging with and reusing the content, as well as what kinds of activity. Not all Github users provide clear information about their role, affiliation, and identity.   Inappropriate Use Cases Like for other altmetrics, Github.com metrics should not be used as a direct measure of quality or impact for a project.   Available Metric Sources Github, PlumX   Transparency All user actions for public repositories are publicly visible.   Website Github.com   Timeframe Real-time updates. The Github site was launched in April 2008.   Last Updated The literature was last scanned in May 2019. This post was last updated June 7, 2019.    </p>",
							url:"https:\/\/metrics-toolkit.github.io\/metrics\/github_forks_collaborators_watchers\/"
						},
						
						{
							value: "Goodreads: Ratings and reviews",
							label: "<p>   Field Field Value     Name Goodreads: Ratings and Reviews   Can Apply To Any book that has been added to the Goodreads catalog   Metric Definition A book’s overall rating is the average of all ratings provided by Goodreads users.   Metric Calculation Mathematical\/quantitative definition of the metric (when available)   Data Sources Goodreads book pages   Appropriate Use Cases Reviews may provide qualitative evidence of impact upon the reviewer(s). Ratings and reviews may be useful as evidence of perceived quality by scholars and the general public.   Limitations Goodreads has more than 55 million users who have written 50 million reviews on 1.5 billion books. Inclusion in Goodreads ratings and reviews is driven entirely by the users rather than the quality or significance of particular books. Most Goodreads users who provided an address are located in the United States, although there are many users across the globe. Only one in five books in PlumX have Goodreads reviews.   Inappropriate Use Cases User ratings and reviews should not be used as measures of scholarly impact, as only a very weak correlation between the two metrics has been found to date (for history monographs).   Available Metric Sources Goodreads book pages, PlumX   Transparency Ratings and book reviews are publicly accessible.   Website Goodreads.com, PlumX Adds Further Book Support with Goodreads Metrics   Timeframe Ratings and reviews are publicly accessible as soon as the user posts them.    </p>",
							url:"https:\/\/metrics-toolkit.github.io\/metrics\/goodreads_ratings_reviews\/"
						},
						
						{
							value: "h-index",
							label: "<p>   Field Field Value     Name h-index   Can Apply To Authors   Metric Definition An author-level metric (although it can also be calculated for any aggregation of publications, e.g. journals, institutions, etc.) calculated from the count of citations to an author’s set of publications.   Metric Calculation In his 2005 paper proposing the h-index, Hirsch describes the measure thusly: “A scientist has index h if h of his or her Np papers have at least h citations each and the other (Np – h) papers have fewer than ≤ h citations each.” For example, an author with an h-index of 6 has at least six journal articles that have each been cited at least six times each.   Data Sources Citation data used to calculate the h-index can be retrieved from Google Scholar, Scopus, Web of Science, Dimensions or any other citation index that includes author- and article-level citation information.   Appropriate Use Cases The h-index has been used as evidence of the scholarly influence of an author’s, or group of authors’, body of work. It is best used in conjunction with other metrics, if at all. Use of the h-index for groups occurs infrequently in practice.   Limitations Many limitations to the h-index have been identified by experts. The h-index varies by discipline due to varying norms of publishing speed and quantity. Since it does not take into account the longevity of a scholar’s career, it benefits more experienced scholars over early-career individuals. The h-index is unable to differentiate between active and inactive scientists, and is biased towards productive researchers in detriment of selective ones. The h-index is also relatively insensitive to highly cited papers. Many have attempted to fix the h-index’s weaknesses with various computational models that, for example, reward highly-cited papers, correct for career length, rank authors’ papers against other papers published in the same year and source, or count just the average citations of the most high-impact “core” of an author’s work. However, none of these improvements upon the h-index have been as widely adopted as the h-index itself.   Inappropriate Use Cases The h-index should not be used as a sole metric of scholarly impact, nor should it be used as a direct measure of quality. The h-index should not be used to rank authors who are in different disciplines or those at different stages of their careers.   Available Metric Sources The h-index can be manually calculated, or you can retrieve it from Google Scholar, Scopus, or Web of Science.   Transparency The formula for calculating the h-index is openly available. See https:\/\/en.wikipedia.org\/wiki\/H-index   Website n\/a   Timeframe All time    Portions of this guide borrow from “Four reasons to stop caring so much about the h-index” by Stacy Konkiel and are reused here under a CC-BY license.\n</p>",
							url:"https:\/\/metrics-toolkit.github.io\/metrics\/h_index\/"
						},
						
						{
							value: "Journal Acceptance Rate",
							label: "<p>   Field Field Value     Name Journal Acceptance Rate   Can Apply To Journal articles, typically in peer-reviewed publications   Metric Definition The percentage of manuscripts accepted for publication, compared to all manuscripts submitted.   Metric Calculation The percentage is calculated by dividing the number of manuscripts accepted for publication in a given year by the number of manuscripts submitted in that same year.   Data Sources Journal editors and publishers   Appropriate Use Cases The acceptance rate for a journal is dependent upon the relative demand for publishing in a particular journal, the peer review processes in place, the mix of invited and unsolicited submissions, and time to publication, among others . As such, it may be a proxy for perceived prestige and demand as compared to availability.   Limitations Many factors unrelated to quality can impact the acceptance rate for any particular journal. Sugimoto et al (2013) found statistically significant differences in article acceptance rates related to discipline, country affiliation of the editor, and number of reviewers per article. Acceptance rates were negatively correlated with citation-based indicators and positively correlated with journal age. Open access journals had statistically significantly higher acceptance rates than subscription only journals.   Inappropriate Use Cases The acceptance rate should not be used as a measure of the quality of a particular manuscript. Manuscript rejection may result from other factors such as a mismatch between the journal’s focus, audience, or format and that of the manuscript. Lower acceptance rates should not be assumed to be the result of higher standards in peer review, according to Haensly et al (2008). Acceptance rate should not be used as a comparative metric across fields or disciplines, according to Haensly et al (2008) and Sugimoto et al (2013).   Available Metric Sources Journal editors, Journal websites, Cabell’s Directories of Publishing Opportunities, and the Modern Language Association Directory of Periodicals   Transparency The data underlying acceptance rates are proprietary. Although some journals make their acceptance rate publicly available, many do not.   Website n\/a   Timeframe Varies    </p>",
							url:"https:\/\/metrics-toolkit.github.io\/metrics\/journal_acceptance_rate\/"
						},
						
						{
							value: "Journal articles",
							label: "<p></p>",
							url:"https:\/\/metrics-toolkit.github.io\/categories\/journal-articles\/"
						},
						
						{
							value: "Journal Impact Factor",
							label: "<p>   Field Field Value     Name Journal Impact Factor   Can Apply To The Journal Impact Factor only applies to journals indexed in the Science Citation Index Expanded and\/or Social Sciences Citation Index by Clarivate Analytics.   Metric Definition The Journal Impact Factor is a measure reflecting the annual average (mean) number of citations to recent articles published in that journal. An essay written by the Institute of Scientific Information (ISI) states “The JCR provides quantitative tools for ranking, evaluating, categorizing, and comparing journals. The impact factor is one of these; it is a measure of the frequency with which the “average article” in a journal has been cited in a particular year or period. The annual JCR impact factor is a ratio between citations and recent citable items published.”   Metric Calculation From Wikipedia : “In any given year, the impact factor of a journal is the number of citations received in that year by articles published in that journal during the two preceding years, divided by the total number of articles published in that journal during the two preceding years.” Citations are counted for all items in the journal though the citations are divided by only the number of “citable items” within the journal, as defined by the creators of the Journal Citation Report (currently Clarivate Analytics). Citable items are restricted by document type to articles and reviews.   Data Sources Citation data used to calculate journal citations is sourced all citations found in the Web of Science Core Collections   Appropriate Use Cases The JIF can be useful in comparing the relative influence of journals within a discipline, as measured by citations. Used appropriately and in conjunction with other metrics, the JIF can be useful in collection development decisions made by librarians. As with all metrics, the JIF should be presented with appropriate context.   Limitations The JIF has been published annually since 1975, and an extensive literature is available on its characteristics, limitations, and common misunderstandings related to its use. Some commonly noted limitations of the JIF include the following.\nThe Journal Impact Factor only applies to journals indexed in the Science Citation Index Expanded and\/or Social Sciences Citation Index by Clarivate Analytics. Journals are reviewed using several criteria  and not all journals are selected for inclusion in the Journal Citation Reports.\nThere is debate about the degree of transparency related to guidelines for the journals and article types selected for inclusion. Some claim there is not enough transparency while others feel that Clarivate’s documentation on the selection process is sufficient.\nA related debate surrounds the differing scope of the numerator and denominator used for calculating the Journal Impact Factor. While the numerator includes all items published in a journal, the denominator includes only those deemed citable by JCR. Items (or document types) included in the denominator are article, review, and proceedings paper. In particular, some take issue with the definition of a “citable item”. The range of JIF values and JCR’s coverage varies by discipline , which makes context such as citation density and rates crucial for interpreting the JIF.\nJournals in the Arts and Humanities Citation Index are reportedly not given a JIF due to the long half-life of citations and references in them.\nResearch published in journals written in languages other than English may be at a disadvantage, as studies have suggested that these journals are not included as often in the JCR.\nJournals published in North America are reportedly disproportionately represented in the sources indexed by Journal Citation Reports.\nReview articles have a higher average citation rate than other types of articles and will influence the net value of the JIF accordingly.   Inappropriate Use Cases As a journal level metric, the JIF should not be used as an indicator for the quality or impact of particular articles or authors. Put another way, the JIF is not statistically representative of (the citations to) individual articles and cannot summarize the quality of an author’s entire body of work.\nAs a retrospective measure of past citations to a journal, the JIF is not a good predictor of whether an individual article will be highly cited. Due to the skewed distribution of citations  (relatively few articles receive most citations, sometimes described as “the long tail”), the use of the mean rather than the median value of citations per article does not offer a reliable prediction for the average number of citations an article can expect to receive.   Available Metric Sources Journal Citation Reports   Transparency The formula for calculating the JIF is public, though there is some debate about the transparency of how journals are selected for inclusion (see above). For those with subscription access to the JCR, a journal’s JCR listing includes a list of the counted items in the denominator. The citation data network and the summarized values used for all metrics are available to subscribers for download.   Website https:\/\/clarivate.com\/products\/journal-citation-reports\/   Timeframe Recently, the JCR has been released about 6 months after the year in question. For example, the JIF for 2016 were released in June 2017.    </p>",
							url:"https:\/\/metrics-toolkit.github.io\/metrics\/journal_impact_factor\/"
						},
						
						{
							value: "Mendeley Readers",
							label: "<p>   Field Field Value     Name Mendeley Readers   Can Apply To Primarily journal articles, but also other types of research outputs, such as web pages, books and conference papers, which users add to their Mendeley reference libraries   Metric Definition The number of Mendeley users that have added a particular document to a Mendeley library. Aggregated demographic information, such as geographic location and discipline for Mendeley readers, are also available.   Metric Calculation Mendeley readership is calculated as the number of unique Mendeley users that have added copies of a particular document to their personal library. Demographic data is user supplied, aggregated, and anonymized.   Data Sources Mendeley personal libraries and user-supplied demographic data.   Appropriate Use Cases Mendeley readership counts are best used to understand early scholarly attention to individual research documents. Studies have found positive correlations between Mendeley readership counts and later citation activity (Li \x26amp; Thelwall, 2012; Zahedi, Costas, and Wouters, 2014; Thelwall, 2018).   Limitations Mendeley’s coverage of science and medicine research is stronger than its coverage of the social sciences and humanities (Mohammadi \x26amp; Thelwall, 2014), especially for books. While several investigations have demonstrated moderate to strong correlations between Mendeley readership counts and later citation activity, this relationship is less significant for articles in the humanities and the social sciences (Mohammadi \x26amp; Thelwall, 2014).    Mendeley has developed a method for clustering different copies of the same document that are stored in different locations around the Web, so the number of readers is calculated across all of the different versions of the same document. Searching Mendeley via a persistent identifier, such as a DOI, can be useful and efficient strategy for retrieving readership count across duplicate files, but it is not 100% accurate.   Inappropriate Use Cases Mendeley readership data should not be used as a direct measure of research quality or impact.   Available Metric Sources The Mendeley Research Catalog and products that incorporate Mendeley readership data such as Scopus, Impactstory Profiles, Altmetric, and PlumX Metrics   Transparency Mendeley readership data is freely available via the Mendeley API, but it only includes readership counts for the three most frequently occurring user types for each document (Haustein \x26amp; Lariviere, 2014). Additionally, Mendeley has started registering its data with the Crossref’s Event Data service. However, it is not possible to see exactly who has bookmarked a reference in Mendeley due to user privacy restrictions. Bookmarks are reported anonymously and in aggregate only.   Website Mendeley, Mendeley developer portal   Timeframe Mendeley was founded in 2007, but in theory research of any age can be added to a Mendeley library.    </p>",
							url:"https:\/\/metrics-toolkit.github.io\/metrics\/mendeley_readers\/"
						},
						
						{
							value: "Metrics",
							label: "<p></p>",
							url:"https:\/\/metrics-toolkit.github.io\/metrics\/"
						},
						
						{
							value: "Monograph holdings",
							label: "<p>   Field Field Value     Name Monograph Holdings   Can Apply To Books   Metric Definition The number of libraries that own (“hold”) a book.   Metric Calculation Holdings counts are derived from national or international union library catalogues, such as OCLC WorldCat. The holdings count is calculated as the number of unique libraries that own a copy of a book.   Data Sources Library book holdings as represented in union catalogs.   Appropriate Use Cases Books may produce benefits and impacts that aren’t well captured by conventional bibliometrics, such as citation counts. Monograph holdings can be used to demonstrate a book’s perceived potential to meet user, teaching, and research needs.   Limitations Monograph holdings counts and contextual data, such as the type of libraries that own a book, do not reveal how, if and how often, and by whom a book is used. Research shows that the use of academic library collections is driven by a small percentage of content. Moreover, monograph holdings provide an incomplete understanding of historical impacts on such areas such as teaching as research, because less current materials are often weeded from library collections.   Inappropriate Use Cases Monograph holdings should not be used as a direct measure of usage, research quality, or impact.   Available Metric Sources Possible sources include WorldCat, Copac, World Bank and IMF collections, PlumX, and various national and regional union catalogs.   Transparency In all of the sources in which monographs holdings data are available, one can trace the details of each library’s ownership information. Note that certain catalogs like WorldCat allow subscribing institutions to withhold their collection information from the public, meaning actual holdings aren’t always fully reflected in publicly-available catalogs.   Website n\/a   Timeframe Monograph holdings data can reflect current library book collections; books of any age that appear in a collection will be included.    </p>",
							url:"https:\/\/metrics-toolkit.github.io\/metrics\/monograph_holdings\/"
						},
						
						{
							value: "Monograph sales and rankings",
							label: "<p>   Field Field Value     Name Monograph Sales and Rankings   Can Apply To Books   Metric Definition Sales figures record the number of times a book has been purchased. Sales ranking is a derivative metric that demonstrates how well a book is selling in comparison to other books.   Metric Calculation Sales figures record the number of recent sales of individual titles across the data sources being tracked by the metric provider. Sales rank is a relative representation of how a book is selling in comparison to all books or other books in the same subject category.   Data Sources Point of sale data from book retailers and consumers.   Appropriate Use Cases Book sales and rankings can be used to demonstrate popular attention to a work.   Limitations Detailed, complete, and historical sales information can be difficult to obtain, as even the most comprehensive sources are limited to the recent past and do not include sales to libraries or used book sales, for example. The most renowned data source for book sales (NPD BookScan) only covers approximately 85% of print trade book sales and only in the U.S.    Book ranks are not representative of cumulative (or historic) sales, but more on how a title has been selling very recently compared to others in the same category.\nMany academic books published by large publishers are targeting captive markets, usually libraries from large universities or research centers. Often, they are compilations of chapters by different authors on very specialised topics. High prices may deter individuals from purchasing edited volumes. While the number of copies sold may be relatively low, the potential audience can be quite large when considering access by library users. | Inappropriate Use Cases | Monograph sales and rankings are not a direct measure of readership, research quality, or impact. | Available Metric Sources | Authors can access their sales data via Amazon, figures are provided by Nielsen BookScan. Additionally, Amazon sales rank data is available from Sales Rank Express. Another provider,NovelRank,was shut down by Amazon in August 2018 on the basis of a breach in their Code of Use. The website will keep historical data available until January 1, 2019. | Transparency | Book sales data and ranking calculations are often proprietary. Purchaser data is anonymous. | Website | n\/a | Timeframe | In most cases, historical sales and ranking data is not available, but books of any age may be included.\n</p>",
							url:"https:\/\/metrics-toolkit.github.io\/metrics\/monograph_sales_rankings\/"
						},
						
						{
							value: "News Mentions",
							label: "<p>   Field Field Value     Name News Mentions   Can Apply To Journal Articles   Metric Definition The number of mainstream online news and magazine outlets that reference a research output.   Metric Calculation Counts are determined by the number of news outlets that reference a specific research output. Metric sources use a variety of methods for recognizing and tracking mentions, such as searching for hyperlinks in news reports and full text searching.   Data Sources Full-text news reports and RSS feeds from online news outlets and magazines. The specific sources and scope of coverage vary by the metric source, especially for non-English-language news outlets.   Appropriate Use Cases News media mentions can demonstrate the reach and attention paid to research, particularly if the source is high profile (e.g. The Economist) or regionally relevant (e.g. Al Jazeera). The type of citing sources (e.g. trade magazines versus widely circulated newspapers) can be used to demonstrate potential engagement with specialized and public audiences.   Limitations Media coverage of research and scientific findings may be skewed towards popular and novel topics, as well as the discipline and type of study being reported (Selvaraj, Borkar \x26amp; Prasad, 2014; Lai \x26amp; Lane, 2009).   Inappropriate Use Cases News media mentions should not be used as a direct measure of quality or impact.   Available Metric Sources Altmetric, PlumX Metrics, Newsflo   Transparency The transparency of news source coverage depends upon the metric provider. Both Altmetric and PlumX provide click through access to the full text of cited media mentions.   Website n\/a   Timeframe Most metric sources began tracking data in 2011 or later; therefore, data is more readily available for recent publications.    </p>",
							url:"https:\/\/metrics-toolkit.github.io\/metrics\/news_mentions\/"
						},
						
						{
							value: "Policy mentions",
							label: "<p>   Field Field Value     Name Policy Mentions   Can Apply To Primarily journal articles and books   Metric Definition The number of times a research output has been cited in policy documents from government bodies or NGOs.   Metric Calculation The total count of citations to a research output in the policy sources being tracked by the metric provider. Each provider tracks differing manually-curated lists of policy bodies.   Data Sources Governmental and non-governmental policy documents   Appropriate Use Cases Policy mentions can be used to demonstrate how research has influenced policy or the course of action in a particular field.   Limitations Research and researchers may influence policy and decision making in indirect ways that are not always trackable via policy citations (Konkiel, 2016). One study found that “less than 0.5% of the papers published in different subject categories are mentioned at least once in policy-related documents” (Haunschild, 2016). In Altmetric, only those books, book chapters, and journal articles with DOIs can be tracked in policy. It is unknown if PlumX Metrics has similar restrictions.   Inappropriate Use Cases Policy mentions should not be used as a direct measure of a research output’s effect on practice.   Available Metric Sources Altmetric, PlumX Metrics   Transparency In relevant providers, one can access the full-text of the policy mentions. However, all providers keep their full list of tracked policy sources private.   Website n\/a   Timeframe Altmetric has been tracking policy mentions since 2013. Plum Analytics (PlumX Metrics) reportedly began tracking policy mentions in 2016, though no publicly available documentation can confirm this.    </p>",
							url:"https:\/\/metrics-toolkit.github.io\/metrics\/policy_mentions\/"
						},
						
						{
							value: "Publons score",
							label: "<p>   Field Field Value     Name Publons Score   Can Apply To Peer reviewed papers and conference proceedings   Metric Definition Publons is a service that allows researchers to track and gain recognition for their peer review and editorial contributions. The Publons Score is an article-level metric that displays the perceived quality and significance of a paper on a scale of 1 to 10, according to peer reviews submitted by registered Publons users. Quality is an indicator of the manuscript’s perceived methodology and rigor, while significance is a measure of its perceived novelty and relevance.   Metric Calculation Peer reviewers assess two aspects of a manuscript on a scale of one to 10 (where one is the lowest score and 10 is the highest). The two aspects assessed are quality and significance, and researchers are asked to consider such questions as “Is the research well designed and executed?” (quality) and “Does the publication offer new insight into the field?” (significance). Publons then calculates an overall score for each aspect and for all participating reviewers using the vector length formula.   Data Sources The ratings for significance and quality are scored by the community of reviewers, editors, and authors on Publons.   Appropriate Use Cases The Publons Score can be used to provide preliminary insight into how a paper is being received and evaluated within a research community.   Limitations Publons coverage is limited to and driven by the researchers who have joined the Publons community (for free). Therefore, differences in quality or significance should not be inferred when comparing papers with and without a Publons Score.   Inappropriate Use Cases The Publons Score is not always an appropriate substitute for the full peer review process, by which manuscripts are improved based upon reviewer commentary over time, and should not be used in isolation as a measure of research impact.   Available Metric Sources Publons, Altmetric   Transparency Publons only displays scores for articles that have been published, and once they appear they are situated next to the article’s Altmetric score and Web of Science citations in the Publons web interface. The number of review scores is provided in the “Metrics” panel of a manuscript’s listing on the network, but the privacy settings associated with any pre-publication review are also applied to the reviewer’s article score. For example, where a journal’s review policy restricts a reviewer from signing their name, then reviewer scores are also anonymous. Researchers who score published articles on Publons are given the option of signing their name to the score   Website Publons   Timeframe Papers published after 2012    </p>",
							url:"https:\/\/metrics-toolkit.github.io\/metrics\/publons_score\/"
						},
						
						{
							value: "Pubpeer comments",
							label: "<p>   Field Field Value     Name Pubpeer Comments   Can Apply To Research papers with a persistent identifier (e.g., digital object identifier (DOI), PubMed ID, or ArXiv ID)   Metric Definition Pubpeer is a post-publication peer review platform. The full text of comments submitted by registered and unregistered users are publicly available.   Metric Calculation Pubpeer comments can be generated by registered and unregistered users for any paper with a DOI, PubMed ID, or ArXiv ID. Full text comments can be retrieved via persistent identifier, keywords, and author names. Comments by registered users are immediately published, while comments by unregistered users are screened for spam content.   Data Sources Comments on articles generated by registered and unregistered Pubpeer users   Appropriate Use Cases Pubpeer comments can be used to provide preliminary insight into how a paper is being received and discussed within a research community.   Limitations While comments can be generated for any paper with a persistent identifier (i.e. DOI, PMID, or ArXiv ID), coverage is limited to and driven by user activity, and not the quality or significance of individual papers. Therefore, differences in quality or significance should not be inferred when comparing papers with and without Pubpeer comments.   Inappropriate Use Cases Pubpeer comments are not a replacement for traditional peer review, and should not be used as a measure of research impact.   Available Metric Sources Pubpeer, Altmetric, Impactstory   Transparency Pubpeer comments are published for registered and unregistered users. Unregistered users are always anonymous. Only the user name, not the specific identity or credentials, of registered users is available.   Website Pubpeer   Timeframe Pubpeer comments have been available since 2012, but comments can be generated for any paper with a DOI, PubMed ID, or ArXiv ID.    </p>",
							url:"https:\/\/metrics-toolkit.github.io\/metrics\/pubpeer_comments\/"
						},
						
						{
							value: "Relative Citation Ratio",
							label: "<p>   Field Field Value     Name Relative Citation Ratio   Can Apply To Journal articles   Metric Definition A field-normalized indicator of influence, used by the NIH for evaluating the relative merits of biomedical research articles   Metric Calculation According to Bloudoff-Indelicato (2015), “The RCR is calculated by dividing the number of citations a paper received by the average number of citations an article usually receives in that field. That number is then benchmarked against the median RCR for all NIH-funded papers. This allows articles to be assessed on the basis of their relevance in their own field, and highly influential articles will be recognized even if they are published in an obscure journal.” The “the average number of citations an article usually receives in that field” is determined by finding the average number of citations for all other articles that are cited alongside the article for which the RCR is being calculated (“an article’s co-citation network”).   Data Sources The NIH iCite database   Appropriate Use Cases To understand the relative scholarly influence that a scientific article has had, as compared to other NIH-funded research. The NIH reportedly uses the RCR for strategic planning and the evaluation of funded research, as a supplement to expert judgement.   Limitations This metric has so far only been applied only to the sciences, as it uses NIH-funded research as its basis of comparison. Further work is under way by researchers to identify its relevance to understanding the influence of arts, humanities, and social sciences research, and possibly that of the influence of science research beyond the biomedical sciences. The benchmarking stage of the RCR calculation provides a potential mechanism to enhance its utility in other areas of research. In common with other citation-based indicators, the RCR has been found to have only low to medium correlation with peer assessments published on F1000 Prime.   Inappropriate Use Cases Like all other citation-based indicators, the RCR cannot measure the quality of research.   Available Metric Sources Dimensions, The NIH’s iCite database   Transparency The metric’s creators describe its calculation in detail in this PLOS Biology article.   Website National Institute of Health: Office of Portfolio Analysis: iCite   Timeframe The RCR accounts for “time since publication” in how it benchmarks an article against other papers, but is not itself time-bound (i.e. calculated over a certain time period like the journal impact factor is).    </p>",
							url:"https:\/\/metrics-toolkit.github.io\/metrics\/relative_citation_ratio\/"
						},
						
						{
							value: "Twitter mentions",
							label: "<p>   Field Field Value     Name Twitter Mentions   Can Apply To Any scholarly product (e.g. journal article) shared on Twitter using a link   Metric Definition Twitter mentions include posts and retweets that reference a trackable scholarly product.   Metric Calculation Counts are determined by the number of registered users that tweet or retweet a post that links to a trackable scholarly product. Depending on the metric data source, mentions may be parsed and presented in different ways. This information can include registered Twitter users’ activity, user demographic information, and user network information, such as follower counts and handles. Note that contrary to popular belief, research outputs do not have to be tweeted using DOI links (e.g. https:\/\/doi.org\/10.1038\/s41590-017-0014-x) in order to be tracked by altmetrics services–nearly all services now are able to identify tweeted scholarly outputs in other ways.   Data Sources Twitter   Appropriate Use Cases Twitter citation counts and contextual demographic and network information can be used to identify the sharing of and reach of information about research on social media, especially within the first weeks of publication (Priem \x26amp; Costello, 2010). Information about who is tweeting and the nature of the tweets is needed to demonstrate the kind and quality of engagement, such as uptake by a scientific audience or interest from a public audience (Thelwall \x26amp; Kousha, 2015).   Limitations Tweet counts can be gamed and there are differences in Twitter use to discuss research across disciplines. Like other metrics, a count of tweets does not reveal the kind of impact an article or other scholarly product had (Thelwall, Tsou, Weingart, Holmberg \x26amp; Haustein, 2013).   Inappropriate Use Cases Twitter citation counts should not be used as a direct measure for a specific kind of impact, such as public engagement.   Available Metric Sources Twitter.com and the Twitter API for recent Twitter activity; Altmetric, PlumX, Impactstory, and Crossref Event Data for historical Twitter activity.   Transparency The transparency of Twitter mentions depends on the source. Some sources are completely auditable in that you can see counts, read all tweets, and access user demographic and network information.   Website Twitter.com   Timeframe Twitter was launched in March 2006. Altmetric, PlumX, and Impactstory began tracking the service in earnest in 2011, meaning that a majority of Twitter data on those sites includes scholarly products created in 2011 and beyond.    </p>",
							url:"https:\/\/metrics-toolkit.github.io\/metrics\/twitter_mentions\/"
						},
						
						{
							value: "Wikipedia citations",
							label: "<p>   Field Field Value     Name Wikipedia Citations   Can Apply To Primarily scholarly publications with a persistent identifier, such as a DOI   Metric Definition References to scholarly outputs in Wikipedia articles, which are linked to in order to support the the entry’s claims.   Metric Calculation The number of times a scholarly output has been referenced in Wikipedia articles. The specific tracking method is dependent on the metric source provider. Altmetric tracks and calculates Wikipedia citations via identifiers and titles that appear in a valid Wikipedia citation, whereas PlumX mines the full text entry for DOIs, PMIDs, and URLs.   Data Sources The exact tracking method and coverage of various Wikipedia languages varies.   Appropriate Use Cases Depending on the context of the citation, Wikipedia references can be interpreted to indicate several kinds of impact, such as a work’s defining influence on a field. To this end, how and where an item is referenced is more important than the count of Wikipedia cations.   Limitations While Wikipedia has guidelines for using and referencing reliable published sources and content criteria, anyone can contribute to the online encyclopedia. Additionally, Wikipedia’s coverage of the published literature is relatively narrow and more evidence about the relationship between Wikipedia citations and other metrics, such article citations counts, is needed. A recent study did find that Open Access and high impact journals were more likely to be on English Wikipedia.   Inappropriate Use Cases Raw Wikipedia citation counts should not be interpreted as a direct measure of quality or impact. The full context of Wikipedia citations must be investigated to understand the kinds of impact the references reflect.   Available Metric Sources Wikipedia, PlumX, Altmetric, Impactstory   Transparency In all of the altmetric services in which Wikipedia citations are available, one can access the full-text of the citing Wikipedia pages.   Website Wikipedia   Timeframe Wikipedia was launched in 2001, but scholarly items can be cited regardless of age.    </p>",
							url:"https:\/\/metrics-toolkit.github.io\/metrics\/wikipedia_citations\/"
						},
						
						{
							value: "Tags",
							label: "<p></p>",
							url:"https:\/\/metrics-toolkit.github.io\/tags\/"
						},
						
					];
					$( "#search" ).autocomplete({
						source: projects
					})
					.data( "ui-autocomplete" )._renderItem = function( ul, item ) {
						return $( "<li>" )
						.append( "<a href=" + item.url + " + \" &quot;\" +  >" + item.value + "</a>" + item.label )
						.appendTo( ul );
					};
					});
				</script>
			</div>
		</div>
	</div>
</div>
<!-- /banner -->
    </header>
    <!-- /header -->
    
    

  
  
  <!-- call to action -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12">
          <div class="section px-3 bg-white shadow text-center">
          <h2 class="mb-4">Explore Metrics</h2>
          <p class="mb-4">Learn more about specific research impact metrics and what they measure.</p>
          
          
          <a href="/metrics" class="btn btn-primary">Explore</a>
          
          
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- /call to action -->
  
  


    <!-- footer -->
<footer class="section pb-4">
  <div class="container">
    <div class="row align-items-center">
      <div class="col-md-8 text-md-left text-center">
       <p class="mb-md-0 mb-4">This site is licensed under a <a href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.</p>
      </div>
      <div class="col-md-4 text-md-right text-center">
        <ul class="list-inline">
          
          <li class="list-inline-item"><a class="text-color d-inline-block p-2" href="https://www.twitter.com/metrics_toolkit"><i class="ti-twitter-alt"></i></a></li>
          
          <li class="list-inline-item"><a class="text-color d-inline-block p-2" href=""><i class=""></i></a></li>
          
        </ul>
      </div>
    </div>
  </div>
</footer>
<!-- /footer -->

<!-- Main Script -->

<script src="https://metrics-toolkit.github.io/js/script.min.js"></script>
  </body>

</html>